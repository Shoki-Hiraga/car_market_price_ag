import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time
import random
from funciton_app.ucarpac_dataget_selectors_edit import process_data
from db_handler import save_to_db, is_recent_url

# å®šç¾©: ãƒ†ãƒ¼ãƒ–ãƒ«å
TABLE_NAME = "market_price_nextage"

# pagenation_selectors ã®ã©ã“ã§ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã•ã›ã‚‹ã‹æŒ‡å®š
select_pagenation_selectors = 2

# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°è¨­å®š
website_url = "https://www.nextage.jp/kaitori/souba/"
start_url = "https://www.nextage.jp/kaitori/souba/"

pagenation_selectors = [
    # ".brand ul:nth-of-type(1) a",
    ".brand li:nth-of-type(10) a",
    "section:nth-of-type(4) .list a",
    "section:nth-of-type(5) td a"
                        ]
dataget_selectors = {
    "maker_name": ".breadcrumb li:nth-of-type(4) a",
    "model_name": ".breadcrumb li:nth-of-type(5) a",
    "grade_name": ".breadcrumb li:nth-of-type(6)",
    "year": "tr:nth-of-type(1) td:nth-of-type(2) a",
    "mileage": "tr:nth-of-type(1) td:nth-of-type(3)",
    "min_price": "tr:nth-of-type(1) td.price",
    "max_price": "tr:nth-of-type(1) td.price",
    "sc_url": "url"
}
pagenations_min = 1
pagenations_max = 10000
delay = random.uniform(1.5, 2.72) 

# ã‚¹ã‚­ãƒƒãƒ—æ¡ä»¶
sc_skip_conditions = [
    {"selector": "div.latest__list--zero", "text": "æ¡ä»¶ã«åˆè‡´ã™ã‚‹å®Ÿç¸¾ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ¡ä»¶ã‚’å¤‰æ›´ã—ã¦å†åº¦æ¤œç´¢ã—ã¦ãã ã•ã„ã€‚"},
    # {"selector": "p.nodata--txt", "text": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“"}
]
# # ã‚¹ã‚­ãƒƒãƒ—æ¡ä»¶ã®ä¸è¦ã®è¨­å®š
# sc_skip_conditions = []

def fetch_page(url):
    print(f"Fetching URL: {url}")  # ãƒ‡ãƒãƒƒã‚°ç”¨
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1"
    ]
    headers = {"User-Agent": random.choice(user_agents)}

    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        # è¤‡æ•°ã®ã‚¹ã‚­ãƒƒãƒ—æ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯
        for condition in sc_skip_conditions:
            skip_element = soup.select_one(condition["selector"])
            if skip_element and condition["text"] in skip_element.get_text():
                print(f"Skipping: {url} due to skip condition match ({condition['selector']} contains '{condition['text']}')")
                return None

        return soup
    except requests.exceptions.HTTPError as e:
        if response.status_code == 404:
            print(f"404 Error for URL: {url}")
        else:
            print(f"HTTP Error for {url}: {e}")
        return None
    except requests.exceptions.RequestException as e:
        return None

    except requests.exceptions.HTTPError as e:
        if response.status_code == 404:
            print(f"404 Error for URL: {url}")
        else:
            print(f"HTTP Error for {url}: {e}")
        return None
    except requests.exceptions.RequestException as e:
        return None

def extract_data(soup, selectors):
    data = {}
    for key, selector in selectors.items():
        if selector == "url":
            continue
        elements = soup.select(selector)
        data[key] = process_data(selector, elements[0].get_text(strip=True)) if elements else None
    return data

def scrape_urls():
    print(f"Starting scrape from: {start_url}\n")
    current_urls = [start_url]
    print(f"Initial URLs: {current_urls}")  # ãƒ‡ãƒãƒƒã‚°ç”¨

    for idx, selector in enumerate(pagenation_selectors):
        next_urls = []

        for url in current_urls:
            soup = fetch_page(url)
            if soup:
                links = [urljoin(url, a['href']) for a in soup.select(selector) if a.get('href')]

                

                # æŒ‡å®šã•ã‚ŒãŸãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚»ãƒ¬ã‚¯ã‚¿ã§ç¯„å›²ã‚’çµåˆ
                if idx == select_pagenation_selectors:
                    for link in links:
                        for page_num in range(pagenations_min, pagenations_max + 1):
                            paginated_url = f"{link.replace('index.html', '')}index{page_num}.html"
                            print(f"Processing paginated URL: {paginated_url}")  # ãƒ‡ãƒãƒƒã‚°ç”¨

                            if is_recent_url(paginated_url, TABLE_NAME):
                                print(f"Skipping: recent URL: {paginated_url}")
                                continue

                            # ãƒšãƒ¼ã‚¸ã‚’å–å¾—ã—ã¦ã€ãã®ä¸­ã®ãƒªãƒ³ã‚¯ã‚’ã•ã‚‰ã«å–å¾—
                            paginated_soup = fetch_page(paginated_url)
                            if not paginated_soup:
                                print(f"Skipping due to error or skip condition: {paginated_url}")
                                break  # ã‚¹ã‚­ãƒƒãƒ—æ¡ä»¶ã‚„404ãŒå‡ºãŸã‚‰æ¬¡ã®ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã¸

                            # æœ€å¾Œã®ã‚»ãƒ¬ã‚¯ã‚¿ã«åŸºã¥ã„ã¦ãƒ‡ãƒ¼ã‚¿å–å¾—ç”¨ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                            dataget_links = [urljoin(website_url, a['href']) for a in paginated_soup.select(pagenation_selectors[-1]) if a.get('href')]
                            print(f"ğŸ” Found {len(dataget_links)} dataget links: {dataget_links}")  # ãƒ‡ãƒãƒƒã‚°è¿½åŠ 

                            for dataget_link in dataget_links:
                                print(f"Fetching data from: {dataget_link}")  # ãƒ‡ãƒãƒƒã‚°ç”¨
                                final_page = fetch_page(dataget_link)
                                if final_page:
                                    data = extract_data(final_page, dataget_selectors)
                                    data["sc_url"] = dataget_link

                                    if any(value is None for value in data.values()):
                                        print(f"Skipping: incomplete data: {data}")
                                        continue

                                    print(f"Saving data: {data}")  # ãƒ‡ãƒãƒƒã‚°ç”¨
                                    save_to_db(data, TABLE_NAME)
                                    time.sleep(delay)
                            time.sleep(delay)
                else:
                    next_urls.extend(links)
            else:
                print(f"Failed to fetch: {url}")  # ãƒ‡ãƒãƒƒã‚°ç”¨
            time.sleep(delay)

        current_urls = next_urls

# å®Ÿè¡Œ
scrape_urls()

